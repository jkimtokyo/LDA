{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkimtokyo/LDA/blob/main/LDA_1URL_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이것은 사본입니다.\n",
        "!pip install konlpy\n",
        "!pip install gensim\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "\n",
        "# SudachiPy와 관련된 라이브러리 설치 (일본어 형태소 분석을 위해)\n",
        "!pip install sudachipy\n",
        "!pip install sudachidict_core\n"
      ],
      "metadata": {
        "id": "A_WzioJmo6VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from konlpy.tag import Okt\n",
        "from gensim import corpora, models\n",
        "from sudachipy import tokenizer\n",
        "from sudachipy import dictionary\n",
        "from collections import defaultdict\n",
        "\n",
        "# Corrected raw URLs for stopwords files\n",
        "kr_stopword_url = 'https://raw.githubusercontent.com/your_github_username/your_repository/branch/kr-stopword.txt'\n",
        "jp_stopword_url = 'https://raw.githubusercontent.com/your_github_username/your_repository/branch/jp-stopword.txt'\n",
        "\n",
        "# Function to download stopwords from the corrected URLs\n",
        "def download_stopwords(url):\n",
        "    response = requests.get(url)\n",
        "    stopwords = response.text.split('\\n')\n",
        "    return [line.strip() for line in stopwords if line.strip()]\n",
        "\n",
        "# Download stopwords using the corrected function\n",
        "kr_stopwords = download_stopwords(kr_stopword_url)\n",
        "jp_stopwords = download_stopwords(jp_stopword_url)\n",
        "\n",
        "# 웹페이지 텍스트 추출 함수\n",
        "def get_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    text = soup.get_text(separator=' ', strip=True)\n",
        "    return text\n",
        "\n",
        "# 형태소 분석 및 불용어 처리 함수\n",
        "def analyze_text(lang, text, stopwords):\n",
        "    processed_text = []\n",
        "    if lang == 'kr':\n",
        "        analyzer = Okt()\n",
        "        tokens = analyzer.nouns(text)\n",
        "    elif lang == 'jp':\n",
        "        analyzer = dictionary.Dictionary().create()\n",
        "        mode = tokenizer.Tokenizer.SplitMode.C\n",
        "        tokens = [m.surface() for m in analyzer.tokenize(text, mode)]\n",
        "    processed_text = [word for word in tokens if word not in stopwords and len(word) > 1]\n",
        "    return processed_text\n",
        "\n",
        "# LDA 모델 생성 및 결과 출력 함수, 주요 단어 15개 추출 및 합산\n",
        "def create_lda_model(processed_docs, num_topics=12, num_words=24):\n",
        "    dictionary = corpora.Dictionary(processed_docs)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "    # 모든 토픽에 걸쳐 단어와 스코어를 합산\n",
        "    word_scores = defaultdict(float)\n",
        "    for idx in range(num_topics):\n",
        "        for word, score in lda_model.show_topic(idx, topn=num_words):\n",
        "            word_scores[word] += score\n",
        "\n",
        "    # 합산된 스코어를 기준으로 내림차순 정렬하여 출력\n",
        "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, score in sorted_words:\n",
        "        print(f\"{word}: {score:.3f}\")\n",
        "\n",
        "# 메인 실행\n",
        "if __name__ == \"__main__\":\n",
        "    lang = input(\"Enter language (kr for Korean, jp for Japanese): \")\n",
        "    url = input(\"Enter URL: \")\n",
        "    text = get_text_from_url(url)\n",
        "    stopwords = kr_stopwords if lang == 'kr' else jp_stopwords\n",
        "    processed_text = analyze_text(lang, text, stopwords)\n",
        "    processed_docs = [processed_text]\n",
        "    create_lda_model(processed_docs, num_topics=12, num_words=24)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_R78l1OUJ8t",
        "outputId": "ae48eb04-993c-40c8-8d65-e3a5aaa05399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter language (kr for Korean, jp for Japanese): kr\n",
            "Enter URL: https://aiopen.etri.re.kr/serviceList\n",
            "기술: 0.130\n",
            "인식: 0.102\n",
            "분석: 0.099\n",
            "제공: 0.086\n",
            "어휘: 0.081\n",
            "정보: 0.076\n",
            "언어: 0.070\n",
            "데이터: 0.068\n",
            "한국어: 0.064\n",
            "서비스: 0.063\n",
            "모델: 0.063\n",
            "학습: 0.063\n",
            "질문: 0.062\n",
            "문장: 0.062\n",
            "평가: 0.061\n",
            "발음: 0.057\n",
            "음성인식: 0.056\n",
            "동영상: 0.054\n",
            "이미지: 0.054\n",
            "관계: 0.052\n",
            "지능: 0.045\n",
            "입력: 0.045\n",
            "사람: 0.041\n",
            "의미: 0.035\n",
            "정답: 0.030\n",
            "속성: 0.013\n",
            "음성: 0.013\n",
            "처리: 0.013\n",
            "개체: 0.013\n",
            "기반: 0.004\n",
            "객체: 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git clone [리포지토리 URL]\n",
        "cd [리포지토리 이름]\n",
        "# 노트북 파일을 리포지토리 디렉토리로 복사\n",
        "git add .\n",
        "git commit -m \"코랩 노트북 추가\"\n",
        "git push\n"
      ],
      "metadata": {
        "id": "ad-H1tgeo7V6",
        "outputId": "6517e5ce-22ea-4114-fe50-2299f893d7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-ea7131f6d1e2>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ea7131f6d1e2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone [리포지토리 URL]\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nes3unxYuZW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}