# 이것은 사본입니다.
!pip install konlpy
!pip install gensim
!pip install beautifulsoup4
!pip install requests

# SudachiPy와 관련된 라이브러리 설치 (일본어 형태소 분석을 위해)
!pip install sudachipy
!pip install sudachidict_core

import requests
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from gensim import corpora, models
from sudachipy import tokenizer
from sudachipy import dictionary
from collections import defaultdict

# Corrected raw URLs for stopwords files
kr_stopword_url = 'https://raw.githubusercontent.com/your_github_username/your_repository/branch/kr-stopword.txt'
jp_stopword_url = 'https://raw.githubusercontent.com/your_github_username/your_repository/branch/jp-stopword.txt'

# Function to download stopwords from the corrected URLs
def download_stopwords(url):
    response = requests.get(url)
    stopwords = response.text.split('\n')
    return [line.strip() for line in stopwords if line.strip()]

# Download stopwords using the corrected function
kr_stopwords = download_stopwords(kr_stopword_url)
jp_stopwords = download_stopwords(jp_stopword_url)

# 웹페이지 텍스트 추출 함수
def get_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    text = soup.get_text(separator=' ', strip=True)
    return text

# 형태소 분석 및 불용어 처리 함수
def analyze_text(lang, text, stopwords):
    processed_text = []
    if lang == 'kr':
        analyzer = Okt()
        tokens = analyzer.nouns(text)
    elif lang == 'jp':
        analyzer = dictionary.Dictionary().create()
        mode = tokenizer.Tokenizer.SplitMode.C
        tokens = [m.surface() for m in analyzer.tokenize(text, mode)]
    processed_text = [word for word in tokens if word not in stopwords and len(word) > 1]
    return processed_text

# LDA 모델 생성 및 결과 출력 함수, 주요 단어 15개 추출 및 합산
def create_lda_model(processed_docs, num_topics=12, num_words=24):
    dictionary = corpora.Dictionary(processed_docs)
    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)

    # 모든 토픽에 걸쳐 단어와 스코어를 합산
    word_scores = defaultdict(float)
    for idx in range(num_topics):
        for word, score in lda_model.show_topic(idx, topn=num_words):
            word_scores[word] += score

    # 합산된 스코어를 기준으로 내림차순 정렬하여 출력
    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
    for word, score in sorted_words:
        print(f"{word}: {score:.3f}")

# 메인 실행
if __name__ == "__main__":
    lang = input("Enter language (kr for Korean, jp for Japanese): ")
    url = input("Enter URL: ")
    text = get_text_from_url(url)
    stopwords = kr_stopwords if lang == 'kr' else jp_stopwords
    processed_text = analyze_text(lang, text, stopwords)
    processed_docs = [processed_text]
    create_lda_model(processed_docs, num_topics=12, num_words=24)
